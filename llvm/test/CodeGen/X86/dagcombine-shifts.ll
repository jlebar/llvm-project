; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-linux-gnu | FileCheck %s

; fold (shl (zext (lshr (A, X))), X) -> (zext (shl (lshr (A, X)), X))

; Canolicalize the sequence shl/zext/lshr performing the zeroextend
; as the last instruction of the sequence.
; This will help DAGCombiner to identify and then fold the sequence
; of shifts into a single AND.
; This transformation is profitable if the shift amounts are the same
; and if there is only one use of the zext.

define i16 @fun1(i8 zeroext %v) {
; CHECK-LABEL: fun1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    andl $-16, %eax
; CHECK-NEXT:    # kill: def $ax killed $ax killed $eax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i8 %v, 4
  %ext = zext i8 %shr to i16
  %shl = shl i16 %ext, 4
  ret i16 %shl
}

define i32 @fun2(i8 zeroext %v) {
; CHECK-LABEL: fun2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    andl $-16, %eax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i8 %v, 4
  %ext = zext i8 %shr to i32
  %shl = shl i32 %ext, 4
  ret i32 %shl
}

define i32 @fun3(i16 zeroext %v) {
; CHECK-LABEL: fun3:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    andl $-16, %eax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i16 %v, 4
  %ext = zext i16 %shr to i32
  %shl = shl i32 %ext, 4
  ret i32 %shl
}

define i64 @fun4(i8 zeroext %v) {
; CHECK-LABEL: fun4:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    andl $-16, %eax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i8 %v, 4
  %ext = zext i8 %shr to i64
  %shl = shl i64 %ext, 4
  ret i64 %shl
}

define i64 @fun5(i16 zeroext %v) {
; CHECK-LABEL: fun5:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    andl $-16, %eax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i16 %v, 4
  %ext = zext i16 %shr to i64
  %shl = shl i64 %ext, 4
  ret i64 %shl
}

define i64 @fun6(i32 zeroext %v) {
; CHECK-LABEL: fun6:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    andl $-16, %eax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i32 %v, 4
  %ext = zext i32 %shr to i64
  %shl = shl i64 %ext, 4
  ret i64 %shl
}

; Don't fold the pattern if we use arithmetic shifts.

define i64 @fun7(i8 zeroext %v) {
; CHECK-LABEL: fun7:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sarb $4, %dil
; CHECK-NEXT:    movzbl %dil, %eax
; CHECK-NEXT:    shlq $4, %rax
; CHECK-NEXT:    retq
entry:
  %shr = ashr i8 %v, 4
  %ext = zext i8 %shr to i64
  %shl = shl i64 %ext, 4
  ret i64 %shl
}

define i64 @fun8(i16 zeroext %v) {
; CHECK-LABEL: fun8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movswl %di, %eax
; CHECK-NEXT:    shrl $4, %eax
; CHECK-NEXT:    movzwl %ax, %eax
; CHECK-NEXT:    shlq $4, %rax
; CHECK-NEXT:    retq
entry:
  %shr = ashr i16 %v, 4
  %ext = zext i16 %shr to i64
  %shl = shl i64 %ext, 4
  ret i64 %shl
}

define i64 @fun9(i32 zeroext %v) {
; CHECK-LABEL: fun9:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    sarl $4, %eax
; CHECK-NEXT:    shlq $4, %rax
; CHECK-NEXT:    retq
entry:
  %shr = ashr i32 %v, 4
  %ext = zext i32 %shr to i64
  %shl = shl i64 %ext, 4
  ret i64 %shl
}

; Don't fold the pattern if there is more than one use of the
; operand in input to the shift left.

define i64 @fun10(i8 zeroext %v) {
; CHECK-LABEL: fun10:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    shrb $4, %dil
; CHECK-NEXT:    movzbl %dil, %ecx
; CHECK-NEXT:    movq %rcx, %rax
; CHECK-NEXT:    shlq $4, %rax
; CHECK-NEXT:    orq %rcx, %rax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i8 %v, 4
  %ext = zext i8 %shr to i64
  %shl = shl i64 %ext, 4
  %add = add i64 %shl, %ext
  ret i64 %add
}

define i64 @fun11(i16 zeroext %v) {
; CHECK-LABEL: fun11:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    # kill: def $edi killed $edi def $rdi
; CHECK-NEXT:    shrl $4, %edi
; CHECK-NEXT:    movq %rdi, %rax
; CHECK-NEXT:    shlq $4, %rax
; CHECK-NEXT:    addq %rdi, %rax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i16 %v, 4
  %ext = zext i16 %shr to i64
  %shl = shl i64 %ext, 4
  %add = add i64 %shl, %ext
  ret i64 %add
}

define i64 @fun12(i32 zeroext %v) {
; CHECK-LABEL: fun12:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    # kill: def $edi killed $edi def $rdi
; CHECK-NEXT:    shrl $4, %edi
; CHECK-NEXT:    movq %rdi, %rax
; CHECK-NEXT:    shlq $4, %rax
; CHECK-NEXT:    addq %rdi, %rax
; CHECK-NEXT:    retq
entry:
  %shr = lshr i32 %v, 4
  %ext = zext i32 %shr to i64
  %shl = shl i64 %ext, 4
  %add = add i64 %shl, %ext
  ret i64 %add
}

; PR17380
; Make sure that the combined dags are legal if we run the DAGCombiner after
; Legalization took place. The add instruction is redundant and increases by
; one the number of uses of the zext. This prevents the transformation from
; firing before dags are legalized and optimized.
; Once the add is removed, the number of uses becomes one and therefore the
; dags are canonicalized. After Legalization, we need to make sure that the
; valuetype for the shift count is legal.
; Verify also that we correctly fold the shl-shr sequence into an
; AND with bitmask.

define void @g(i32 %a) {
; CHECK-LABEL: g:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $edi killed $edi def $rdi
; CHECK-NEXT:    andl $-4, %edi
; CHECK-NEXT:    jmp f # TAILCALL
  %b = lshr i32 %a, 2
  %c = zext i32 %b to i64
  %d = add i64 %c, 1
  %e = shl i64 %c, 2
  tail call void @f(i64 %e)
  ret void
}

declare void @f(i64)

; The *_select tests below check that we do the following transformation:
;
;  shift lhs, (select cond, constant1, constant2) -->
;  select cond, (shift lhs, constant1), (shift lhs, constant2)
;
; When updating these testcases, ensure that there are two shift instructions
; in the result and that they take immediates rather than registers.
define i32 @shl_select(i32 %x, i1 %cond) {
; CHECK-LABEL: shl_select:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    movl %edi, %ecx
; CHECK-NEXT:    shrl $3, %ecx
; CHECK-NEXT:    shrl $6, %eax
; CHECK-NEXT:    testb $1, %sil
; CHECK-NEXT:    cmovnel %ecx, %eax
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 3, i32 6
  %ret = lshr i32 %x, %shift_amnt
  ret i32 %ret
}

define i32 @ashr_select(i32 %x, i1 %cond) {
; CHECK-LABEL: ashr_select:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    movl %edi, %ecx
; CHECK-NEXT:    sarl $3, %ecx
; CHECK-NEXT:    sarl $6, %eax
; CHECK-NEXT:    testb $1, %sil
; CHECK-NEXT:    cmovnel %ecx, %eax
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 3, i32 6
  %ret = ashr i32 %x, %shift_amnt
  ret i32 %ret
}

define i32 @lshr_select(i32 %x, i1 %cond) {
; CHECK-LABEL: lshr_select:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    movl %edi, %ecx
; CHECK-NEXT:    shrl $3, %ecx
; CHECK-NEXT:    shrl $6, %eax
; CHECK-NEXT:    testb $1, %sil
; CHECK-NEXT:    cmovnel %ecx, %eax
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 3, i32 6
  %ret = lshr i32 %x, %shift_amnt
  ret i32 %ret
}

; Check that we don't perform the folding described in shl_select when the
; shift width is used other than as an input to the shift instruction.
;
; When updating this testcase, check that there's exactly one shrl instruction
; generated.
declare void @i32_foo(i32)
define i32 @shl_select_not_folded_if_shift_amnt_is_used(i32 %x, i1 %cond) {
; CHECK-LABEL: shl_select_not_folded_if_shift_amnt_is_used:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushq %rbp
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    .cfi_def_cfa_offset 24
; CHECK-NEXT:    pushq %rax
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    .cfi_offset %rbx, -24
; CHECK-NEXT:    .cfi_offset %rbp, -16
; CHECK-NEXT:    movl %edi, %ebx
; CHECK-NEXT:    notb %sil
; CHECK-NEXT:    movzbl %sil, %eax
; CHECK-NEXT:    andl $1, %eax
; CHECK-NEXT:    leal 3(%rax,%rax,2), %ebp
; CHECK-NEXT:    movl %ebp, %edi
; CHECK-NEXT:    callq i32_foo
; CHECK-NEXT:    movl %ebp, %ecx
; CHECK-NEXT:    shrl %cl, %ebx
; CHECK-NEXT:    movl %ebx, %eax
; CHECK-NEXT:    addq $8, %rsp
; CHECK-NEXT:    .cfi_def_cfa_offset 24
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    popq %rbp
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 3, i32 6
  call void @i32_foo(i32 %shift_amnt)
  %ret = lshr i32 %x, %shift_amnt
  ret i32 %ret
}

; Check that we don't perfrm the folding described in shl_select when one of
; the shift widths is not a constant.
;
; When updating these testcases, check that there's exactly one shrl
; instruction generated in each.
define i32 @shl_select_not_folded_if_shift_amnt_is_nonconstant_1(i32 %x, i32 %a, i1 %cond) {
; CHECK-LABEL: shl_select_not_folded_if_shift_amnt_is_nonconstant_1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    testb $1, %dl
; CHECK-NEXT:    movl $6, %ecx
; CHECK-NEXT:    cmovnel %esi, %ecx
; CHECK-NEXT:    # kill: def $cl killed $cl killed $ecx
; CHECK-NEXT:    shrl %cl, %eax
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 %a, i32 6
  %ret = lshr i32 %x, %shift_amnt
  ret i32 %ret
}

define i32 @shl_select_not_folded_if_shift_amnt_is_nonconstant_2(i32 %x, i32 %a, i1 %cond) {
; CHECK-LABEL: shl_select_not_folded_if_shift_amnt_is_nonconstant_2:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    testb $1, %dl
; CHECK-NEXT:    movl $3, %ecx
; CHECK-NEXT:    cmovel %esi, %ecx
; CHECK-NEXT:    # kill: def $cl killed $cl killed $ecx
; CHECK-NEXT:    shrl %cl, %eax
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 3, i32 %a
  %ret = lshr i32 %x, %shift_amnt
  ret i32 %ret
}

define i32 @shl_select_not_folded_if_shift_amnt_is_nonconstant_3(i32 %x, i32 %a, i32 %b, i1 %cond) {
; CHECK-LABEL: shl_select_not_folded_if_shift_amnt_is_nonconstant_3:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    testb $1, %cl
; CHECK-NEXT:    cmovel %edx, %esi
; CHECK-NEXT:    movl %esi, %ecx
; CHECK-NEXT:    shrl %cl, %eax
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 %a, i32 %b
  %ret = lshr i32 %x, %shift_amnt
  ret i32 %ret
}

; Check that we don't perform the folding described in shl_select when the
; operand is a vector, because x86 vector shifts don't go faster when the shift
; width is a known constant.
;
; When updating this testcase, check that there's exactly one shift instruction
; generated.
define <4 x i32> @shr_select_not_folded_for_vector_shifts1(<4 x i32> %x, i1 %cond) {
; CHECK-LABEL: shr_select_not_folded_for_vector_shifts1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    notb %dil
; CHECK-NEXT:    movzbl %dil, %eax
; CHECK-NEXT:    andl $1, %eax
; CHECK-NEXT:    leal 3(%rax,%rax,2), %eax
; CHECK-NEXT:    movd %eax, %xmm1
; CHECK-NEXT:    psrld %xmm1, %xmm0
; CHECK-NEXT:    retq
  %shift_amnt = select i1 %cond, i32 3, i32 6
  %vshift_amnt0 = insertelement <4 x i32> undef, i32 %shift_amnt, i32 0
  %vshift_amnt1 = insertelement <4 x i32> %vshift_amnt0, i32 %shift_amnt, i32 1
  %vshift_amnt2 = insertelement <4 x i32> %vshift_amnt1, i32 %shift_amnt, i32 2
  %vshift_amnt3 = insertelement <4 x i32> %vshift_amnt2, i32 %shift_amnt, i32 3
  %ret = lshr <4 x i32> %x, %vshift_amnt3
  ret <4 x i32> %ret
}
